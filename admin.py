import streamlit as st
import os
import shutil
import time
import datetime
import pandas as pd
import re
import tempfile
import sys
import warnings
import json

# ë¬¸ì„œ ë³€í™˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pymupdf4llm
import mammoth
import markdownify
import olefile

# LangChain & Chroma ê´€ë ¨
from langchain_chroma import Chroma
from langchain.retrievers import EnsembleRetriever
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_core.documents import Document

# core ëª¨ë“ˆ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ê²½ë¡œ í™•ì¸)
try:
    from core.config import PERSIST_DIRECTORY
    from core.llm import get_embeddings
except ImportError:
    # core ëª¨ë“ˆì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ í•˜ë“œì½”ë”© (í…ŒìŠ¤íŠ¸ìš©)
    PERSIST_DIRECTORY = "./chroma_db"
    from langchain_huggingface import HuggingFaceEmbeddings
    def get_embeddings():
        return HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")

# ------------------------------------------------------------------
# ê¸°ë³¸ê²½ë¡œ ì„¤ì • (ì ˆëŒ€ ê²½ë¡œë¡œ ê³ ì •í•˜ì—¬ main.pyì™€ ë¶ˆì¼ì¹˜ ë°©ì§€)
# ------------------------------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SHARED_DIR = os.path.join(BASE_DIR, "shared")
if not os.path.exists(SHARED_DIR):
    os.makedirs(SHARED_DIR)
# [config íŒŒì¼ ê²½ë¡œ ì •ì˜]
CONFIG_FILE = os.path.join(SHARED_DIR, "system_config.json")

st.set_page_config(page_title="ğŸ›  ê´€ë¦¬ì ì½˜ì†”", layout="wide")
st.title("ğŸ›  ê·œì • Â· ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê´€ë¦¬ì ì½˜ì†”")

# ------------------------------------------------------------------
# 1. í•µì‹¬ í•¨ìˆ˜ ì •ì˜ (í…ìŠ¤íŠ¸ ì •ì œ ë° ë¬¸ì„œ ì²˜ë¦¬)
# ------------------------------------------------------------------

def clean_markdown_text(text):
    """
    ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê¸°í˜¸, ë¹ˆ í‘œ, ê³¼ë„í•œ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    if not isinstance(text, str):
        return ""

    # 1. ë¬´ì˜ë¯¸í•œ í‘œ í–‰ ì œê±° (ì˜ˆ: | | | | | )
    text = re.sub(r'^[|\s-]+$', '', text, flags=re.MULTILINE)
    
    # 2. ì—°ì†ëœ ì¤„ë°”ê¿ˆ ë° ê³µë°± ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    
    # 3. ë§ˆí¬ë‹¤ìš´ ì´ë¯¸ì§€/ë§í¬ íƒœê·¸ ì œê±°
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'\[.*?\]\(.*?\)', '', text)
    
    # 4. íŠ¹ìˆ˜ë¬¸ì ë…¸ì´ì¦ˆ ì œê±°
    text = text.replace("~~", "")
    
    return text.strip()

def extract_hwp_text(hwp_path):
    """HWP íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        f = olefile.OleFileIO(hwp_path)
        encoded_text = f.openstream("PrvText").read()
        decoded_text = encoded_text.decode("utf-16le")
        return decoded_text
    except Exception as e:
        return f"[HWP ì˜¤ë¥˜] ë³€í™˜ ì‹¤íŒ¨: {e}"

def process_file_to_docs(file, source_name):
    """íŒŒì¼ì„ ì½ì–´ ì²­í¬(Chunk) ë‹¨ìœ„ì˜ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ì™„ì „ ë°©ì–´ ëª¨ë“œ)"""
    file_ext = os.path.splitext(file.name)[1].lower()
    
    # [ìˆ˜ì • 1] ë³€ìˆ˜ë¥¼ í•¨ìˆ˜ ì‹œì‘ ì§€ì ì—ì„œ ë¯¸ë¦¬ ì„ ì–¸í•˜ì—¬ UnboundLocalError ë°©ì§€
    md_text = "" 
    
    with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
        tmp.write(file.getvalue())
        tmp_path = tmp.name

    try:
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
        try:
            if file_ext == ".pdf":
                md_text = pymupdf4llm.to_markdown(tmp_path)
            elif file_ext == ".docx":
                result = mammoth.convert_to_html(tmp_path)
                # Mammoth ê²°ê³¼ ê²€ì¦
                html_content = result.value if (result and result.value) else ""
                md_text = markdownify.markdownify(html_content, heading_style="ATX", strip=['img'])
            elif file_ext in [".hwp", ".hwpx"]:
                raw_text = extract_hwp_text(tmp_path) 
                md_text = f"# {source_name} ë³¸ë¬¸\n\n{raw_text}"
            else:
                return []
        except Exception as e:
            st.error(f"íŒŒì¼ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({file.name}): {e}")
            md_text = "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”

        # [ìˆ˜ì • 2] ì¶”ì¶œëœ ë‚´ìš©ì´ Noneì¸ ê²½ìš°ì— ëŒ€í•œ 2ì¤‘ ë°©ì–´
        if md_text is None:
            md_text = ""

        # 2. í…ìŠ¤íŠ¸ ì •ì œ
        md_text = clean_markdown_text(md_text)
        
        if len(md_text.strip()) < 10:
            return []
        
        # 3. í—¤ë” ì²˜ë¦¬ ë° ì²­í‚¹ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        md_text = re.sub(r'(^|\n)(ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°)', r'\n# \2', md_text)
        
        headers_to_split_on = [("#", "Article_Title")]
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        header_splits = markdown_splitter.split_text(md_text)
        
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        
        final_docs = []
        for doc in header_splits:
            content = str(doc.page_content) if doc.page_content else ""
            if len(content.strip()) < 10:
                continue
                
            splits = text_splitter.split_text(content)
            for split_content in splits:
                # [ìˆ˜ì • 3] Document ìƒì„± ì‹œ page_contentê°€ ì ˆëŒ€ Noneì´ ë˜ì§€ ì•Šë„ë¡ ê°•ì œ ë³€í™˜
                new_doc = Document(
                    page_content=str(split_content), 
                    metadata={
                        "source": source_name,
                        "Article_Title": str(doc.metadata.get("Article_Title", "ì¼ë°˜")),
                        "file_type": file_ext
                    }
                )
                final_docs.append(new_doc)
                
        return final_docs
        
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

# ------------------------------------------------------------------
# 2. ì‚¬ì´ë“œë°” UI (íŒŒì¼ ì—…ë¡œë“œ ë° ê´€ë¦¬)
# ------------------------------------------------------------------
with st.sidebar:
    st.header("âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
    
    # [ê°œì„ 1] Ollama ëª¨ë¸ ì„ íƒ ê¸°ëŠ¥
    st.subheader("ğŸ¤– LLM ëª¨ë¸ ì„ íƒ")
    ollama_models = [
        "korean-gemma2:latest",
        "korean-llama3:latest",
        "my-korean-llama3:latest",
        "gemma2:latest",
        "llama3:latest",
        "gemma3:4b",
        "nomic-embed-text:latest" 
    ]
    # 1. í˜„ì¬ ì €ì¥ëœ ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸° (ì´ˆê¸°ê°’ ì„¤ì •)
    current_index = 1 # ê¸°ë³¸ê°’ (korean-llama3)
    if os.path.exists(CONFIG_FILE):
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                saved_config = json.load(f)
                saved_model = saved_config.get("selected_model", "korean-llama3:latest")
                if saved_model in ollama_models:
                    current_index = ollama_models.index(saved_model)
        except:
            pass # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ìœ ì§€

    # 2. ì„ íƒë°•ìŠ¤ í‘œì‹œ
    selected_model = st.selectbox(
        "ì‚¬ìš©í•  ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”:",
        options=ollama_models,
        index=current_index
    )
    
    # 3. ë³€ê²½ ê°ì§€ ë° íŒŒì¼ ì €ì¥
    # ì´ì „ ì„ íƒê³¼ ë‹¤ë¥´ë©´ íŒŒì¼ì— ì”ë‹ˆë‹¤.
    if "selected_model" not in st.session_state:
        st.session_state.selected_model = ollama_models[current_index]

    if st.session_state.selected_model != selected_model:
        st.session_state.selected_model = selected_model
        
        # [í•µì‹¬] JSON íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ main.pyì™€ ê³µìœ 
        with open(CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump({"selected_model": selected_model}, f)
            
        st.toast(f"âœ… ëª¨ë¸ì´ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤: {selected_model}")
    st.divider()
    st.header("ğŸ“‚ ë°ì´í„° ê´€ë¦¬")
    
    # --- [ì„¹ì…˜ 1] ê·œì • íŒŒì¼ í•™ìŠµ ---
    st.subheader("1. ê·œì • íŒŒì¼ í•™ìŠµ")
    uploaded_files = st.file_uploader(
        "PDF, DOCX, HWP íŒŒì¼ ì—…ë¡œë“œ", 
        type=["pdf", "docx", "hwp"], 
        accept_multiple_files=True
    )
    
    if st.button("ğŸš€ DB í•™ìŠµ ì‹œì‘", use_container_width=True):
        if uploaded_files:
            with st.spinner("ë¬¸ì„œ ë¶„ì„ ë° ë²¡í„° DB ì €ì¥ ì¤‘..."):
                all_docs = []
                for file in uploaded_files:
                    try:
                        docs = process_file_to_docs(file, file.name)
                        if docs:
                            all_docs.extend(docs)
                            st.toast(f"âœ… {file.name} ì²˜ë¦¬ ì™„ë£Œ ({len(docs)} chunks)")
                        else:
                            st.error(f"âš ï¸ {file.name}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                    except Exception as e:
                        st.error(f"âŒ {file.name} ì˜¤ë¥˜: {e}")
                
                if all_docs:
                    vectorstore = Chroma(
                        persist_directory=PERSIST_DIRECTORY, 
                        embedding_function=get_embeddings()
                    )
                    vectorstore.add_documents(all_docs)
                    st.success(f"ğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ! (ì´ {len(all_docs)}ê°œ ë°ì´í„°)")
                    time.sleep(1)
                    st.rerun()
        else:
            st.warning("íŒŒì¼ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")

    st.divider()
    
    # --- [ì„¹ì…˜ 2] ì‹œìŠ¤í…œ ì´ˆê¸°í™” ---
    st.subheader("2. ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    if st.button("ğŸ—‘ï¸ ê·œì • DB ì „ì²´ ì‚­ì œ", type="primary", use_container_width=True):
        if os.path.exists(PERSIST_DIRECTORY):
            shutil.rmtree(PERSIST_DIRECTORY)
            st.success("DBê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
            time.sleep(1)
            st.rerun()
        else:
            st.info("ì‚­ì œí•  DBê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.divider()

    # --- [ì„¹ì…˜ 3] ìƒí™©ë³´ê³  ì—‘ì…€ ì—…ë¡œë“œ ---
    st.subheader("3. ìƒí™©ë³´ê³  ë°ì´í„° ì—…ë¡œë“œ")
    uploaded_file = st.file_uploader(
        "íŒŒì¼ì„ ë“œë˜ê·¸í•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”.",
        type=["csv", "xls", "xlsx"],
        help="CSV ë˜ëŠ” ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì‹œìŠ¤í…œì— ì¦‰ì‹œ ë°˜ì˜ë©ë‹ˆë‹¤."
    )

    if uploaded_file is not None:
        try:
            filename = uploaded_file.name.lower()
            df = None

            # 2. íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì²˜ë¦¬
            if filename.endswith(".csv"):
                # í•œê¸€ ê¹¨ì§ ë°©ì§€ë¥¼ ìœ„í•´ ì¸ì½”ë”© ìˆœì°¨ ì‹œë„
                try:
                    df = pd.read_csv(uploaded_file, encoding="utf-8-sig")
                except:
                    uploaded_file.seek(0)  # íŒŒì¼ í¬ì¸í„° ì´ˆê¸°í™”
                    df = pd.read_csv(uploaded_file, encoding="cp949")
            
            elif filename.endswith(".xls"):
                df = pd.read_excel(uploaded_file, engine="xlrd")
                
            elif filename.endswith(".xlsx"):
                df = pd.read_excel(uploaded_file, engine="openpyxl")

            # 3. ë°ì´í„° ì €ì¥ ë¡œì§
            if df is not None:
                # shared í´ë” ë‚´ risk_df.pklë¡œ ì €ì¥ (ë©”ì¸ ë¶„ì„ê¸° ì—°ë™ìš©)
                FILE_PATH = os.path.join(SHARED_DIR, "risk_df.pkl")
                df.to_pickle(FILE_PATH)
                
                st.sidebar.success(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {filename}")
                st.sidebar.info(f"ë°ì´í„° ê°œìˆ˜: {len(df)}í–‰")
                
                # ì—…ë¡œë“œ í›„ í™”ë©´ ê°±ì‹  (ë°˜ì˜ í™•ì¸ìš©)
                if st.sidebar.button("ë°ì´í„° ì¦‰ì‹œ ê°±ì‹ "):
                    st.rerun()
            else:
                st.sidebar.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")

        except Exception as e:
            st.sidebar.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ------------------------------------------------------------------
# 3. ë©”ì¸ í™”ë©´ (ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë° í”¼ë“œë°±)
# ------------------------------------------------------------------
st.header("ğŸ“Š í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ")

# [1] ê·œì • ë°ì´í„° ìƒíƒœ (ì „ì²´ ë„ˆë¹„ ì‚¬ìš©)
st.subheader("ğŸ“š ê·œì • ë°ì´í„° ê´€ë¦¬ (Chroma DB)")

if os.path.exists(PERSIST_DIRECTORY):
    try:
        vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=get_embeddings())
        collection = vectorstore.get() 
        
        total_docs = len(collection['ids']) if collection else 0
        
        if total_docs > 0:
            file_stats = {}
            for idx, meta in enumerate(collection['metadatas']):
                src = meta.get('source', 'ì•Œìˆ˜ì—†ìŒ')
                doc_content = collection['documents'][idx]
                doc_id = collection['ids'][idx]
                
                if src not in file_stats:
                    file_stats[src] = {
                        "ids": [], 
                        "count": 0, 
                        "preview": doc_content[:50].replace("\n", " ") + "..."
                    }
                file_stats[src]["ids"].append(doc_id)
                file_stats[src]["count"] += 1

            df_data = []
            for src, info in file_stats.items():
                df_data.append({
                    "íŒŒì¼ëª…": src,
                    "ì²­í¬(Chunk) ìˆ˜": info["count"],
                    "ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (Article)": info["preview"]
                })
            
            df_files = pd.DataFrame(df_data)

            c1, c2 = st.columns([1, 1])
            c1.metric("ì´ í•™ìŠµëœ íŒŒì¼", f"{len(df_files)} ê°œ")
            c2.metric("ì´ ë²¡í„° ì²­í¬ ìˆ˜", f"{total_docs} ê°œ")
            
            st.markdown("##### ğŸ“‹ í•™ìŠµëœ íŒŒì¼ ëª©ë¡ ìƒì„¸")
            st.dataframe(
                df_files, 
                use_container_width=True, 
                hide_index=True,
                column_config={
                    "ì²­í¬(Chunk) ìˆ˜": st.column_config.NumberColumn(format="%d ê°œ"),
                    "ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (Article)": st.column_config.TextColumn(width="large")
                }
            )

            st.markdown("##### ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œ ê´€ë¦¬")
            files_to_delete = st.multiselect(
                "ì‚­ì œí•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥):",
                options=df_files["íŒŒì¼ëª…"].tolist()
            )
            
            if files_to_delete:
                st.warning(f"ì„ íƒí•œ {len(files_to_delete)}ê°œ íŒŒì¼ì„ DBì—ì„œ ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
                if st.button("ğŸ—‘ï¸ ì„ íƒ í•­ëª© ì˜êµ¬ ì‚­ì œ", type="primary"):
                    try:
                        total_deleted_ids = []
                        for file_name in files_to_delete:
                            ids = file_stats[file_name]["ids"]
                            total_deleted_ids.extend(ids)
                        
                        if total_deleted_ids:
                            vectorstore.delete(ids=total_deleted_ids)
                            st.success(f"âœ… ì´ {len(total_deleted_ids)}ê°œì˜ ì²­í¬ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            time.sleep(1.5)
                            st.rerun()
                    except Exception as e:
                        st.error(f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        else:
            st.info("í•™ìŠµëœ ê·œì • ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° 0ê±´)")

    except Exception as e:
        st.error(f"DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.caption("DB íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
else:
    st.info("í•™ìŠµëœ ê·œì • ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

st.divider()

# [2] ìƒí™©ë³´ê³  ë°ì´í„° ìƒíƒœ
st.subheader("ğŸ“ˆ ìƒí™©ë³´ê³  ë°ì´í„° (Excel)")

RISK_FILE_PATH = os.path.join(SHARED_DIR, "risk_df.pkl")

if os.path.exists(RISK_FILE_PATH):
    try:
        saved_df = pd.read_pickle(RISK_FILE_PATH) 
        st.metric("ì €ì¥ëœ ìƒí™©ë³´ê³  ê±´ìˆ˜", f"{len(saved_df)} ê±´")
        
        st.markdown("**ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5ê±´):**")
        st.dataframe(saved_df.head(), use_container_width=True)
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
else:
    st.info("ì—…ë¡œë“œëœ ìƒí™©ë³´ê³  ì—‘ì…€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì—‘ì…€ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

st.divider()

# ==================================================================
# admin.py ìˆ˜ì • ì½”ë“œ
# [3] í”¼ë“œë°± ë£¨í”„ (Human-in-the-Loop) ë¶€ë¶„ ì „ì²´ êµì²´
# ==================================================================

st.divider()

st.subheader("ğŸ“ ì‚¬ìš©ì í”¼ë“œë°± ê´€ë¦¬ (Human-in-the-Loop)")
st.caption("ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ ê²€í† í•˜ì—¬ AIë¥¼ ê°•í™”í•™ìŠµ ì‹œí‚¤ê³ , ê³¼ê±° í•™ìŠµ ì´ë ¥ì„ í™•ì¸í•©ë‹ˆë‹¤.")

# ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
feedback_file = os.path.join(SHARED_DIR, "feedback_log.csv")

if os.path.exists(feedback_file):
    try:
        # ë°ì´í„° ë¡œë“œ
        df_fb = pd.read_csv(feedback_file)
        
        # íƒ­ ë¶„ë¦¬: [ê²€í†  ëŒ€ê¸°] vs [í•™ìŠµ ì™„ë£Œ ì´ë ¥]
        tab_review, tab_history = st.tabs(["ğŸ”¥ ê²€í†  ë° í•™ìŠµ (Pending)", "ğŸ“œ í•™ìŠµ ì™„ë£Œ ë¡œê·¸ (History)"])
        
        # ----------------------------------------------------------
        # TAB 1: ê²€í†  ë° í•™ìŠµ (ê¸°ì¡´ ê¸°ëŠ¥)
        # ----------------------------------------------------------
        with tab_review:
            # Pending ìƒíƒœë§Œ í•„í„°ë§
            pending_df = df_fb[df_fb['Status'] == 'Pending']
            
            if pending_df.empty:
                st.info("ğŸ‰ í˜„ì¬ ëŒ€ê¸° ì¤‘ì¸ í”¼ë“œë°±ì´ ì—†ìŠµë‹ˆë‹¤. (ëª¨ë‘ ì²˜ë¦¬ë¨)")
            else:
                st.write(f"ì´ **{len(pending_df)}ê±´**ì˜ ìƒˆë¡œìš´ í”¼ë“œë°±ì´ ëŒ€ê¸° ì¤‘ì…ë‹ˆë‹¤.")
                
                for index, row in pending_df.iterrows():
                    with st.expander(f"[{row['Rating']}] {str(row['Question'])[:40]}...", expanded=True):
                        c1, c2 = st.columns(2)
                        with c1:
                            st.info(f"ğŸ¤– **AI ê¸°ì¡´ ë‹µë³€:**\n\n{row['AI_Answer']}")
                        with c2:
                            st.error(f"ğŸ‘¤ **ì‚¬ìš©ì ìˆ˜ì • ì œì•ˆ:**\n\n{row['User_Correction']}")
                        
                        btn_col1, btn_col2 = st.columns([1, 5])
                        with btn_col1:
                            # í•™ìŠµ ë²„íŠ¼
                            if st.button(f"âœ… DB í•™ìŠµ ë°˜ì˜", key=f"train_{index}"):
                                try:
                                    raw_q = row['Question'] if pd.notna(row['Question']) else ""
                                    raw_cor = row['User_Correction'] if pd.notna(row['User_Correction']) else ""
                                    
                                    # [ë°ì´í„° í†µì¼] ê²€ìƒ‰ ì—”ì§„ì´ 'Article_Title'ì—ì„œ ì •ë‹µì„ ì°¾ê¸° ì‰½ê²Œ êµ¬ì„±
                                    unified_title = f"ì² ë„ì•ˆì „ë²• {raw_q[:15]}" 
                                    
                                    enhanced_content = f"ì§ˆë¬¸: {raw_q}\nì •ë‹µ: {raw_cor}\nì„¤ëª…: í˜„ì¥ ì „ë¬¸ê°€ ê²€ì¦ì„ ê±°ì¹œ ì² ë„ì•ˆì „ë²• ì‹œí–‰ê·œì¹™ ì¤€ìˆ˜ ì‚¬í•­ì…ë‹ˆë‹¤."

                                    new_doc = Document(
                                        page_content=enhanced_content, 
                                        metadata={
                                            "source": "Expert_Knowledge", # ì¶œì²˜ í†µì¼
                                            "type": "feedback",
                                            # "reward_score": 5.0,           # ê²€ìƒ‰ ê°€ì¤‘ì¹˜ë¥¼ ìœ„í•´ ë†’ì€ ë³´ìƒ ì ìˆ˜ ë¶€ì—¬
                                            # "Article_Title": unified_title, # ê¸°ì¡´ ê·œì •ê³¼ ë§¤ì¹­ë˜ë„ë¡ ì œëª© ë¶€ì—¬
                                            "Article_Title": f"[ê²€ì¦] {raw_q[:15]}", # ì œëª© í•„ë“œ ê°•í™”(v1.3)
                                            "reward_score": 10.0, # ê²€ìƒ‰ ìˆœìœ„ ìµœìƒë‹¨ ë³´ì¥ ì ìˆ˜(v1.3)
                                            "timestamp": str(datetime.datetime.now())
                                        }
                                    )
                                    
                                    # DB ì €ì¥
                                    vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=get_embeddings())
                                    vectorstore.add_documents([new_doc])
                                    
                                    # CSV ì—…ë°ì´íŠ¸ (Status: Applied)
                                    df_fb.at[index, 'Status'] = 'Applied'
                                    df_fb.to_csv(feedback_file, index=False, encoding='utf-8-sig')
                                    
                                    st.success(f"âœ… '{unified_title}' ì§€ì‹ì´ ê°•í™”í•™ìŠµ ì •ì±…ì— ë°˜ì˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
                                    st.rerun()
                                    
                                except Exception as e:
                                    st.error(f"í•™ìŠµ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        
                        with btn_col2:
                            # ê¸°ê°(ì‚­ì œ) ë²„íŠ¼
                            if st.button("ğŸ—‘ï¸ ë¬´ì‹œ(ì‚­ì œ)", key=f"del_{index}"):
                                df_fb.at[index, 'Status'] = 'Ignored'
                                df_fb.to_csv(feedback_file, index=False)
                                st.rerun()

                            # RL ê´€ì : ì˜ëª»ëœ ë‹µë³€ íŒ¨í„´ ê¸°ë¡
                            negative_doc = Document(
                                page_content=f"[ë¶€ì • í”¼ë“œë°±]\nì§ˆë¬¸:{row['Question']}\nì˜ëª»ëœ ì‘ë‹µ:{row['AI_Answer']}",
                                metadata={
                                    "type": "negative_feedback",
                                    "reward_score": -1.0,
                                    "confidence": 0.9,
                                    "source": "ê´€ë¦¬ì ê¸°ê°"
                                }
                            )
                            vectorstore.add_documents([negative_doc])
                            st.rerun()

        # ----------------------------------------------------------
        # TAB 2: í•™ìŠµ ì™„ë£Œ ë¡œê·¸ (ì‹ ê·œ ê¸°ëŠ¥)
        # ----------------------------------------------------------
        with tab_history:
            # Applied ìƒíƒœë§Œ í•„í„°ë§ (ìµœì‹ ìˆœ ì •ë ¬)
            history_df = df_fb[df_fb['Status'] == 'Applied'].sort_values(by='Timestamp', ascending=False)
            
            if history_df.empty:
                st.info("ì•„ì§ í•™ìŠµì— ë°˜ì˜ëœ ì´ë ¥ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.success(f"ì´ **{len(history_df)}ê±´**ì˜ ì§€ì‹ì´ AIì— ì¶”ê°€ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ê°€ë…ì„±ì„ ìœ„í•´ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ (í•„ìš”í•œ ì»¬ëŸ¼ë§Œ)
                display_cols = ['Timestamp', 'Question', 'User_Correction', 'Rating']
                
                # ì»¬ëŸ¼ëª… í•œê¸€í™” (ë³´ê¸° ì¢‹ê²Œ)
                display_df = history_df[display_cols].copy()
                display_df.columns = ['ì²˜ë¦¬ì¼ì‹œ(ì ‘ìˆ˜)', 'ì§ˆë¬¸ ë‚´ìš©', 'í•™ìŠµì‹œí‚¨ ì •ë‹µ', 'í‰ê°€']
                
                st.dataframe(
                    display_df, 
                    use_container_width=True,
                    hide_index=True
                )
                
                # ìƒì„¸ ë³´ê¸° (ì•„ì½”ë””ì–¸ í˜•íƒœ)
                with st.expander("ğŸ” ìƒì„¸ ì´ë ¥ ì¡°íšŒ (í´ë¦­)"):
                    for i, row in history_df.iterrows():
                        st.markdown(f"**[{row['Timestamp']}] {row['Question']}**")
                        st.text(f"ğŸ‘‰ í•™ìŠµëœ ì •ë‹µ: {row['User_Correction']}")
                        st.divider()

    except Exception as e:
        st.error(f"í”¼ë“œë°± ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

else:
    st.info("ì•„ì§ ìˆ˜ì§‘ëœ í”¼ë“œë°± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (shared/feedback_log.csv íŒŒì¼ ì—†ìŒ)")