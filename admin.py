import streamlit as st
import os
import shutil
import time
import pandas as pd
import re
import tempfile
import sys
import warnings

# ë¬¸ì„œ ë³€í™˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pymupdf4llm
import mammoth
import markdownify
import olefile

# LangChain & Chroma ê´€ë ¨
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_core.documents import Document

# core ëª¨ë“ˆ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ê²½ë¡œ í™•ì¸)
try:
    from core.config import PERSIST_DIRECTORY
    from core.llm import get_embeddings
except ImportError:
    # core ëª¨ë“ˆì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ í•˜ë“œì½”ë”© (í…ŒìŠ¤íŠ¸ìš©)
    PERSIST_DIRECTORY = "./chroma_db"
    from langchain_huggingface import HuggingFaceEmbeddings
    def get_embeddings():
        return HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")

# ------------------------------------------------------------------
# ê¸°ë³¸ ì„¤ì •
# ------------------------------------------------------------------
st.set_page_config(page_title="ğŸ›  ê´€ë¦¬ì ì½˜ì†”", layout="wide")
st.title("ğŸ›  ê·œì • Â· ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê´€ë¦¬ì ì½˜ì†”")

# ------------------------------------------------------------------
# 1. í•µì‹¬ í•¨ìˆ˜ ì •ì˜ (í…ìŠ¤íŠ¸ ì •ì œ ë° ë¬¸ì„œ ì²˜ë¦¬)
# ------------------------------------------------------------------

def clean_markdown_text(text):
    """
    ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê¸°í˜¸, ë¹ˆ í‘œ, ê³¼ë„í•œ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    if not isinstance(text, str):
        return ""

    # 1. ë¬´ì˜ë¯¸í•œ í‘œ í–‰ ì œê±° (ì˜ˆ: | | | | | )
    # íŒŒì´í”„(|), ê³µë°±(\s), í•˜ì´í”ˆ(-)ìœ¼ë¡œë§Œ êµ¬ì„±ëœ ì¤„ì„ ì‚­ì œ
    text = re.sub(r'^[|\s-]+$', '', text, flags=re.MULTILINE)
    
    # 2. ì—°ì†ëœ ì¤„ë°”ê¿ˆ ë° ê³µë°± ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)  # 3ì¤„ ì´ìƒ ê³µë°± -> 2ì¤„ë¡œ
    text = re.sub(r'[ \t]+', ' ', text)     # ì—°ì†ëœ ìŠ¤í˜ì´ìŠ¤/íƒ­ -> ê³µë°± 1ê°œ
    
    # 3. ë§ˆí¬ë‹¤ìš´ ì´ë¯¸ì§€/ë§í¬ íƒœê·¸ ì œê±° (í…ìŠ¤íŠ¸ ë¶„ì„ì— ë°©í•´ë¨)
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'\[.*?\]\(.*?\)', '', text)
    
    # 4. íŠ¹ìˆ˜ë¬¸ì ë…¸ì´ì¦ˆ ì œê±° (ë¬¼ê²°í‘œ ë“±)
    text = text.replace("~~", "")
    
    return text.strip()

def extract_hwp_text(hwp_path):
    """HWP íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        f = olefile.OleFileIO(hwp_path)
        encoded_text = f.openstream("PrvText").read()
        decoded_text = encoded_text.decode("utf-16le")
        return decoded_text
    except Exception as e:
        return f"[HWP ì˜¤ë¥˜] ë³€í™˜ ì‹¤íŒ¨: {e}"

def process_file_to_docs(file, source_name):
    """íŒŒì¼ì„ ì½ì–´ ì²­í¬(Chunk) ë‹¨ìœ„ì˜ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    file_ext = os.path.splitext(file.name)[1].lower()
    
    # ì„ì‹œ íŒŒì¼ ìƒì„±
    with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
        tmp.write(file.getvalue())
        tmp_path = tmp.name

    try:
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        md_text = ""
        if file_ext == ".pdf":
            md_text = pymupdf4llm.to_markdown(tmp_path)
        elif file_ext == ".docx":
            result = mammoth.convert_to_html(tmp_path)
            html = result.value
            md_text = markdownify.markdownify(html, heading_style="ATX", strip=['img'])
        elif file_ext in [".hwp", ".hwpx"]:
            raw_text = extract_hwp_text(tmp_path) 
            md_text = f"# {source_name} ë³¸ë¬¸\n\n{raw_text}"
        else:
            return []
        
        # 2. [ì¤‘ìš”] í…ìŠ¤íŠ¸ ê°•ë ¥ ì •ì œ
        md_text = clean_markdown_text(md_text)
        
        # 3. í—¤ë” ì²˜ë¦¬ (ì œNì¡° -> # ì œNì¡°)
        md_text = re.sub(r'(^|\n)(ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°)', r'\n# \2', md_text)
        
        # 4. ì²­í¬ ë¶„í•  (Chunking)
        headers_to_split_on = [("#", "Article_Title")]
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        header_splits = markdown_splitter.split_text(md_text)
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, 
            chunk_overlap=200
        )
        
        final_docs = []
        for doc in header_splits:
            if len(doc.page_content.strip()) < 10:
                continue
                
            splits = text_splitter.split_text(doc.page_content)
            for split_content in splits:
                if re.match(r'^[|\s-]+$', split_content):
                    continue

                new_doc = Document(
                    page_content=split_content,
                    metadata={
                        "source": source_name,
                        "Article_Title": doc.metadata.get("Article_Title", "ì¼ë°˜"),
                        "file_type": file_ext
                    }
                )
                final_docs.append(new_doc)
                
        return final_docs
        
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

# ------------------------------------------------------------------
# 2. ì‚¬ì´ë“œë°” UI (íŒŒì¼ ì—…ë¡œë“œ ë° ê´€ë¦¬)
# ------------------------------------------------------------------
with st.sidebar:
    st.header("ğŸ“‚ ë°ì´í„° ê´€ë¦¬")
    
    # --- [ì„¹ì…˜ 1] ê·œì • íŒŒì¼ í•™ìŠµ ---
    st.subheader("1. ê·œì • íŒŒì¼ í•™ìŠµ")
    uploaded_files = st.file_uploader(
        "PDF, DOCX, HWP íŒŒì¼ ì—…ë¡œë“œ", 
        type=["pdf", "docx", "hwp"], 
        accept_multiple_files=True
    )
    
    if st.button("ğŸš€ DB í•™ìŠµ ì‹œì‘", use_container_width=True):
        if uploaded_files:
            with st.spinner("ë¬¸ì„œ ë¶„ì„ ë° ë²¡í„° DB ì €ì¥ ì¤‘..."):
                all_docs = []
                for file in uploaded_files:
                    try:
                        docs = process_file_to_docs(file, file.name)
                        if docs:
                            all_docs.extend(docs)
                            st.toast(f"âœ… {file.name} ì²˜ë¦¬ ì™„ë£Œ ({len(docs)} chunks)")
                        else:
                            st.error(f"âš ï¸ {file.name}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                    except Exception as e:
                        st.error(f"âŒ {file.name} ì˜¤ë¥˜: {e}")
                
                if all_docs:
                    vectorstore = Chroma(
                        persist_directory=PERSIST_DIRECTORY, 
                        embedding_function=get_embeddings()
                    )
                    vectorstore.add_documents(all_docs)
                    st.success(f"ğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ! (ì´ {len(all_docs)}ê°œ ë°ì´í„°)")
                    time.sleep(1)
                    st.rerun()
        else:
            st.warning("íŒŒì¼ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")

    st.divider()
    
    # --- [ì„¹ì…˜ 2] ì‹œìŠ¤í…œ ì´ˆê¸°í™” ---
    st.subheader("2. ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    if st.button("ğŸ—‘ï¸ ê·œì • DB ì „ì²´ ì‚­ì œ", type="primary", use_container_width=True):
        if os.path.exists(PERSIST_DIRECTORY):
            shutil.rmtree(PERSIST_DIRECTORY)
            st.success("DBê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
            time.sleep(1)
            st.rerun()
        else:
            st.info("ì‚­ì œí•  DBê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.divider()

    # --- [ì„¹ì…˜ 3] ìƒí™©ë³´ê³  ì—‘ì…€ ì—…ë¡œë“œ (ë³µêµ¬ëœ ë¶€ë¶„) ---
    st.subheader("3. ìƒí™©ë³´ê³  ë°ì´í„° ì—…ë¡œë“œ")
    excel = st.file_uploader(
        "ìƒí™©ë³´ê³  ì—‘ì…€ ì—…ë¡œë“œ (.xls, .xlsx)",
        type=["xls", "xlsx"]
    )

    if excel is not None:
        # íŒŒì¼ í¬ì¸í„° ì´ˆê¸°í™”
        excel.seek(0)
        try:
            filename = excel.name.lower()
            if filename.endswith(".xls"):
                # .xls ì§€ì›ì„ ìœ„í•´ xlrd ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš” (pip install xlrd)
                df = pd.read_excel(excel, engine="xlrd")
            elif filename.endswith(".xlsx"):
                df = pd.read_excel(excel, engine="openpyxl")
            else:
                st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—‘ì…€ í˜•ì‹ì…ë‹ˆë‹¤.")
                st.stop() 

            st.success(f"ì—‘ì…€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(df)}í–‰)")
            
            # shared í´ë”ì— í”¼í´ íŒŒì¼ë¡œ ì €ì¥ (Main ì•±ê³¼ ê³µìœ )
            BASE_DIR = os.path.dirname(os.path.abspath(__file__))
            SHARED_DIR = os.path.join(BASE_DIR, "shared")
            os.makedirs(SHARED_DIR, exist_ok=True)
            FILE_PATH = os.path.join(SHARED_DIR, "risk_df.pkl")

            df.to_pickle(FILE_PATH)
            st.success("âœ… ìƒí™©ë³´ê³  ë°ì´í„°ê°€ ê³µìš© ì €ì¥ì†Œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            st.error(f"ì—‘ì…€ ë¡œë“œ ì‹¤íŒ¨: {e}")


# ------------------------------------------------------------------
# 3. ë©”ì¸ í™”ë©´ (ìƒíƒœ ëª¨ë‹ˆí„°ë§)
# ------------------------------------------------------------------
st.header("ğŸ“Š í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ")

# [1] ê·œì • ë°ì´í„° ìƒíƒœ (ì „ì²´ ë„ˆë¹„ ì‚¬ìš©)
st.subheader("ğŸ“š ê·œì • ë°ì´í„° ê´€ë¦¬ (Chroma DB)")

if os.path.exists(PERSIST_DIRECTORY):
    try:
        # ChromaDB ë¡œë“œ
        vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=get_embeddings())
        collection = vectorstore.get() # ì €ì¥ëœ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        
        total_docs = len(collection['ids']) if collection else 0
        
        if total_docs > 0:
            # ---------------------------------------------------------
            # 1. íŒŒì¼ë³„ í†µê³„ ë°ì´í„° ê°€ê³µ (íŒŒì¼ëª…, ì²­í¬ìˆ˜, ë¯¸ë¦¬ë³´ê¸° ë“±)
            # ---------------------------------------------------------
            file_stats = {}
            
            # ë©”íƒ€ë°ì´í„°ì™€ ë¬¸ì„œë¥¼ ìˆœíšŒí•˜ë©° ê·¸ë£¹í™”
            for idx, meta in enumerate(collection['metadatas']):
                src = meta.get('source', 'ì•Œìˆ˜ì—†ìŒ')
                doc_content = collection['documents'][idx]
                doc_id = collection['ids'][idx]
                
                if src not in file_stats:
                    file_stats[src] = {
                        "ids": [],          # ì‚­ì œ ì‹œ í•„ìš”í•œ ID ë¦¬ìŠ¤íŠ¸
                        "count": 0,         # ì²­í¬ ê°œìˆ˜
                        "preview": doc_content[:50].replace("\n", " ") + "..." # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²« ì²­í¬ ê¸°ì¤€)
                    }
                
                file_stats[src]["ids"].append(doc_id)
                file_stats[src]["count"] += 1

            # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
            df_data = []
            for src, info in file_stats.items():
                df_data.append({
                    "íŒŒì¼ëª…": src,
                    "ì²­í¬(Chunk) ìˆ˜": info["count"],
                    "ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (Article)": info["preview"]
                })
            
            df_files = pd.DataFrame(df_data)

            # ---------------------------------------------------------
            # 2. ìƒíƒœ í‘œì‹œ ë° í…Œì´ë¸” ì¶œë ¥
            # ---------------------------------------------------------
            c1, c2 = st.columns([1, 1])
            c1.metric("ì´ í•™ìŠµëœ íŒŒì¼", f"{len(df_files)} ê°œ")
            c2.metric("ì´ ë²¡í„° ì²­í¬ ìˆ˜", f"{total_docs} ê°œ")
            
            st.markdown("##### ğŸ“‹ í•™ìŠµëœ íŒŒì¼ ëª©ë¡ ìƒì„¸")
            st.dataframe(
                df_files, 
                use_container_width=True, 
                hide_index=True,
                column_config={
                    "ì²­í¬(Chunk) ìˆ˜": st.column_config.NumberColumn(format="%d ê°œ"),
                    "ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (Article)": st.column_config.TextColumn(width="large")
                }
            )

            # ---------------------------------------------------------
            # 3. íŒŒì¼ ì‚­ì œ ê¸°ëŠ¥ (Multiselect + Button)
            # ---------------------------------------------------------
            st.divider()
            st.markdown("##### ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œ ê´€ë¦¬")
            
            # ì‚­ì œí•  íŒŒì¼ ì„ íƒ
            files_to_delete = st.multiselect(
                "ì‚­ì œí•  íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš” (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥):",
                options=df_files["íŒŒì¼ëª…"].tolist()
            )
            
            if files_to_delete:
                st.warning(f"ì„ íƒí•œ {len(files_to_delete)}ê°œ íŒŒì¼ì„ DBì—ì„œ ì˜êµ¬ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
                if st.button("ğŸ—‘ï¸ ì„ íƒ í•­ëª© ì˜êµ¬ ì‚­ì œ", type="primary"):
                    try:
                        # ì‚­ì œ ë¡œì§
                        total_deleted_ids = []
                        for file_name in files_to_delete:
                            ids = file_stats[file_name]["ids"]
                            total_deleted_ids.extend(ids)
                        
                        if total_deleted_ids:
                            vectorstore.delete(ids=total_deleted_ids)
                            # vectorstore.persist() # ìµœì‹  Chroma ë²„ì „ì€ ìë™ ì €ì¥ë˜ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´ í™•ì¸ í•„ìš”
                            
                            st.success(f"âœ… ì´ {len(total_deleted_ids)}ê°œì˜ ì²­í¬(íŒŒì¼ {len(files_to_delete)}ê°œ)ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            time.sleep(1.5) # ë©”ì‹œì§€ ë³´ì—¬ì¤„ ì‹œê°„ í™•ë³´
                            st.rerun() # í™”ë©´ ìƒˆë¡œê³ ì¹¨
                            
                    except Exception as e:
                        st.error(f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        else:
            st.info("í•™ìŠµëœ ê·œì • ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° 0ê±´)")

    except Exception as e:
        st.error(f"DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.caption("DB íŒŒì¼ì´ ì†ìƒë˜ì—ˆê±°ë‚˜ ê²½ë¡œê°€ ì˜ëª»ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
else:
    st.info("í•™ìŠµëœ ê·œì • ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

st.divider()

# [2] ìƒí™©ë³´ê³  ë°ì´í„° ìƒíƒœ (ì „ì²´ ë„ˆë¹„ ì‚¬ìš©)
st.subheader("ğŸ“ˆ ìƒí™©ë³´ê³  ë°ì´í„° (Excel)")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SHARED_DIR = os.path.join(BASE_DIR, "shared")
FILE_PATH = os.path.join(SHARED_DIR, "risk_df.pkl")

if os.path.exists(FILE_PATH):
    try:
        saved_df = pd.read_pickle(FILE_PATH)
        st.metric("ì €ì¥ëœ ìƒí™©ë³´ê³  ê±´ìˆ˜", f"{len(saved_df)} ê±´")
        
        st.markdown("**ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5ê±´):**")
        st.dataframe(saved_df.head(), use_container_width=True)
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
else:
    st.info("ì—…ë¡œë“œëœ ìƒí™©ë³´ê³  ì—‘ì…€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì—‘ì…€ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")