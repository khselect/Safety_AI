import streamlit as st
import os
import shutil
import time
import pandas as pd
import re
import tempfile
import sys
import warnings

# ë¬¸ì„œ ë³€í™˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pymupdf4llm
import mammoth
import markdownify
import olefile

# LangChain & Chroma ê´€ë ¨
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter
from langchain_core.documents import Document

# core ëª¨ë“ˆ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ê²½ë¡œ í™•ì¸)
try:
    from core.config import PERSIST_DIRECTORY
    from core.llm import get_embeddings
except ImportError:
    # core ëª¨ë“ˆì´ ì—†ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ í•˜ë“œì½”ë”© (í…ŒìŠ¤íŠ¸ìš©)
    PERSIST_DIRECTORY = "./chroma_db"
    from langchain_huggingface import HuggingFaceEmbeddings
    def get_embeddings():
        return HuggingFaceEmbeddings(
            model_name="jhgan/ko-sroberta-multitask",
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")

# ------------------------------------------------------------------
# ê¸°ë³¸ ì„¤ì •
# ------------------------------------------------------------------
st.set_page_config(page_title="ğŸ›  ê´€ë¦¬ì ì½˜ì†”", layout="wide")
st.title("ğŸ›  ê·œì • Â· ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê´€ë¦¬ì ì½˜ì†”")

# ------------------------------------------------------------------
# 1. í•µì‹¬ í•¨ìˆ˜ ì •ì˜ (í…ìŠ¤íŠ¸ ì •ì œ ë° ë¬¸ì„œ ì²˜ë¦¬)
# ------------------------------------------------------------------

def clean_markdown_text(text):
    """
    ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê¸°í˜¸, ë¹ˆ í‘œ, ê³¼ë„í•œ ê³µë°±ì„ ì œê±°í•©ë‹ˆë‹¤.
    """
    if not isinstance(text, str):
        return ""

    # 1. ë¬´ì˜ë¯¸í•œ í‘œ í–‰ ì œê±° (ì˜ˆ: | | | | | )
    # íŒŒì´í”„(|), ê³µë°±(\s), í•˜ì´í”ˆ(-)ìœ¼ë¡œë§Œ êµ¬ì„±ëœ ì¤„ì„ ì‚­ì œ
    text = re.sub(r'^[|\s-]+$', '', text, flags=re.MULTILINE)
    
    # 2. ì—°ì†ëœ ì¤„ë°”ê¿ˆ ë° ê³µë°± ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)  # 3ì¤„ ì´ìƒ ê³µë°± -> 2ì¤„ë¡œ
    text = re.sub(r'[ \t]+', ' ', text)     # ì—°ì†ëœ ìŠ¤í˜ì´ìŠ¤/íƒ­ -> ê³µë°± 1ê°œ
    
    # 3. ë§ˆí¬ë‹¤ìš´ ì´ë¯¸ì§€/ë§í¬ íƒœê·¸ ì œê±° (í…ìŠ¤íŠ¸ ë¶„ì„ì— ë°©í•´ë¨)
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'\[.*?\]\(.*?\)', '', text)
    
    # 4. íŠ¹ìˆ˜ë¬¸ì ë…¸ì´ì¦ˆ ì œê±° (ë¬¼ê²°í‘œ ë“±)
    text = text.replace("~~", "")
    
    return text.strip()

def extract_hwp_text(hwp_path):
    """HWP íŒŒì¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        f = olefile.OleFileIO(hwp_path)
        encoded_text = f.openstream("PrvText").read()
        decoded_text = encoded_text.decode("utf-16le")
        return decoded_text
    except Exception as e:
        return f"[HWP ì˜¤ë¥˜] ë³€í™˜ ì‹¤íŒ¨: {e}"

def process_file_to_docs(file, source_name):
    """íŒŒì¼ì„ ì½ì–´ ì²­í¬(Chunk) ë‹¨ìœ„ì˜ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
    file_ext = os.path.splitext(file.name)[1].lower()
    
    # ì„ì‹œ íŒŒì¼ ìƒì„±
    with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
        tmp.write(file.getvalue())
        tmp_path = tmp.name

    try:
        # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
        md_text = ""
        if file_ext == ".pdf":
            md_text = pymupdf4llm.to_markdown(tmp_path)
        elif file_ext == ".docx":
            result = mammoth.convert_to_html(tmp_path)
            html = result.value
            md_text = markdownify.markdownify(html, heading_style="ATX", strip=['img'])
        elif file_ext in [".hwp", ".hwpx"]:
            raw_text = extract_hwp_text(tmp_path) 
            md_text = f"# {source_name} ë³¸ë¬¸\n\n{raw_text}"
        else:
            return []
        
        # 2. [ì¤‘ìš”] í…ìŠ¤íŠ¸ ê°•ë ¥ ì •ì œ
        md_text = clean_markdown_text(md_text)
        
        # 3. í—¤ë” ì²˜ë¦¬ (ì œNì¡° -> # ì œNì¡°)
        md_text = re.sub(r'(^|\n)(ì œ\s*\d+(?:ì˜\d+)?\s*ì¡°)', r'\n# \2', md_text)
        
        # 4. ì²­í¬ ë¶„í•  (Chunking)
        headers_to_split_on = [("#", "Article_Title")]
        markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
        header_splits = markdown_splitter.split_text(md_text)
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, 
            chunk_overlap=200
        )
        
        final_docs = []
        for doc in header_splits:
            if len(doc.page_content.strip()) < 10:
                continue
                
            splits = text_splitter.split_text(doc.page_content)
            for split_content in splits:
                if re.match(r'^[|\s-]+$', split_content):
                    continue

                new_doc = Document(
                    page_content=split_content,
                    metadata={
                        "source": source_name,
                        "Article_Title": doc.metadata.get("Article_Title", "ì¼ë°˜"),
                        "file_type": file_ext
                    }
                )
                final_docs.append(new_doc)
                
        return final_docs
        
    finally:
        if os.path.exists(tmp_path):
            os.remove(tmp_path)

# ------------------------------------------------------------------
# 2. ì‚¬ì´ë“œë°” UI (íŒŒì¼ ì—…ë¡œë“œ ë° ê´€ë¦¬)
# ------------------------------------------------------------------
with st.sidebar:
    st.header("ğŸ“‚ ë°ì´í„° ê´€ë¦¬")
    
    # --- [ì„¹ì…˜ 1] ê·œì • íŒŒì¼ í•™ìŠµ ---
    st.subheader("1. ê·œì • íŒŒì¼ í•™ìŠµ")
    uploaded_files = st.file_uploader(
        "PDF, DOCX, HWP íŒŒì¼ ì—…ë¡œë“œ", 
        type=["pdf", "docx", "hwp"], 
        accept_multiple_files=True
    )
    
    if st.button("ğŸš€ DB í•™ìŠµ ì‹œì‘", use_container_width=True):
        if uploaded_files:
            with st.spinner("ë¬¸ì„œ ë¶„ì„ ë° ë²¡í„° DB ì €ì¥ ì¤‘..."):
                all_docs = []
                for file in uploaded_files:
                    try:
                        docs = process_file_to_docs(file, file.name)
                        if docs:
                            all_docs.extend(docs)
                            st.toast(f"âœ… {file.name} ì²˜ë¦¬ ì™„ë£Œ ({len(docs)} chunks)")
                        else:
                            st.error(f"âš ï¸ {file.name}: í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
                    except Exception as e:
                        st.error(f"âŒ {file.name} ì˜¤ë¥˜: {e}")
                
                if all_docs:
                    vectorstore = Chroma(
                        persist_directory=PERSIST_DIRECTORY, 
                        embedding_function=get_embeddings()
                    )
                    vectorstore.add_documents(all_docs)
                    st.success(f"ğŸ‰ ì „ì²´ í•™ìŠµ ì™„ë£Œ! (ì´ {len(all_docs)}ê°œ ë°ì´í„°)")
                    time.sleep(1)
                    st.rerun()
        else:
            st.warning("íŒŒì¼ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”.")

    st.divider()
    
    # --- [ì„¹ì…˜ 2] ì‹œìŠ¤í…œ ì´ˆê¸°í™” ---
    st.subheader("2. ì‹œìŠ¤í…œ ì´ˆê¸°í™”")
    if st.button("ğŸ—‘ï¸ ê·œì • DB ì „ì²´ ì‚­ì œ", type="primary", use_container_width=True):
        if os.path.exists(PERSIST_DIRECTORY):
            shutil.rmtree(PERSIST_DIRECTORY)
            st.success("DBê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ í•™ìŠµì‹œì¼œì£¼ì„¸ìš”.")
            time.sleep(1)
            st.rerun()
        else:
            st.info("ì‚­ì œí•  DBê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.divider()

    # --- [ì„¹ì…˜ 3] ìƒí™©ë³´ê³  ì—‘ì…€ ì—…ë¡œë“œ (ë³µêµ¬ëœ ë¶€ë¶„) ---
    st.subheader("3. ìƒí™©ë³´ê³  ë°ì´í„° ì—…ë¡œë“œ")
    excel = st.file_uploader(
        "ìƒí™©ë³´ê³  ì—‘ì…€ ì—…ë¡œë“œ (.xls, .xlsx)",
        type=["xls", "xlsx"]
    )

    if excel is not None:
        # íŒŒì¼ í¬ì¸í„° ì´ˆê¸°í™”
        excel.seek(0)
        try:
            filename = excel.name.lower()
            if filename.endswith(".xls"):
                # .xls ì§€ì›ì„ ìœ„í•´ xlrd ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš” (pip install xlrd)
                df = pd.read_excel(excel, engine="xlrd")
            elif filename.endswith(".xlsx"):
                df = pd.read_excel(excel, engine="openpyxl")
            else:
                st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—‘ì…€ í˜•ì‹ì…ë‹ˆë‹¤.")
                st.stop() 

            st.success(f"ì—‘ì…€ ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(df)}í–‰)")
            
            # shared í´ë”ì— í”¼í´ íŒŒì¼ë¡œ ì €ì¥ (Main ì•±ê³¼ ê³µìœ )
            BASE_DIR = os.path.dirname(os.path.abspath(__file__))
            SHARED_DIR = os.path.join(BASE_DIR, "shared")
            os.makedirs(SHARED_DIR, exist_ok=True)
            FILE_PATH = os.path.join(SHARED_DIR, "risk_df.pkl")

            df.to_pickle(FILE_PATH)
            st.success("âœ… ìƒí™©ë³´ê³  ë°ì´í„°ê°€ ê³µìš© ì €ì¥ì†Œì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            st.error(f"ì—‘ì…€ ë¡œë“œ ì‹¤íŒ¨: {e}")


# ------------------------------------------------------------------
# 3. ë©”ì¸ í™”ë©´ (ìƒíƒœ ëª¨ë‹ˆí„°ë§)
# ------------------------------------------------------------------
st.header("ğŸ“Š í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ")

# [1] ê·œì • ë°ì´í„° ìƒíƒœ (ì „ì²´ ë„ˆë¹„ ì‚¬ìš©)
st.subheader("ğŸ“š ê·œì • ë°ì´í„° (Chroma DB)")

if os.path.exists(PERSIST_DIRECTORY):
    try:
        vectorstore = Chroma(persist_directory=PERSIST_DIRECTORY, embedding_function=get_embeddings())
        collection = vectorstore.get()
        doc_count = len(collection['ids']) if collection else 0
        
        st.metric("í•™ìŠµëœ ê·œì • ì²­í¬ ìˆ˜", f"{doc_count} ê°œ")
        
        if doc_count > 0:
            sources = list(set([m['source'] for m in collection['metadatas'] if m.get('source')]))
            st.markdown("**í•™ìŠµëœ íŒŒì¼ ëª©ë¡:**")
            st.dataframe(pd.DataFrame(sources, columns=["íŒŒì¼ëª…"]), use_container_width=True)
            
            # (ì„ íƒì‚¬í•­) ë°ì´í„° ìƒ˜í”Œ í™•ì¸ì´ í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œ
            # with st.expander("ğŸ” ë°ì´í„° ìƒ˜í”Œ í™•ì¸ (ìµœê·¼ 5ê°œ)"):
            #     for i in range(min(5, doc_count)):
            #         st.info(f"**[{collection['metadatas'][i].get('source')}]**\n\n{collection['documents'][i][:200]}...")
                    
    except Exception as e:
        st.error(f"DB ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
else:
    st.info("í•™ìŠµëœ ê·œì • ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

st.divider() # ì„¹ì…˜ êµ¬ë¶„ì„ 

# [2] ìƒí™©ë³´ê³  ë°ì´í„° ìƒíƒœ (ì „ì²´ ë„ˆë¹„ ì‚¬ìš©)
st.subheader("ğŸ“ˆ ìƒí™©ë³´ê³  ë°ì´í„° (Excel)")

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SHARED_DIR = os.path.join(BASE_DIR, "shared")
FILE_PATH = os.path.join(SHARED_DIR, "risk_df.pkl")

if os.path.exists(FILE_PATH):
    try:
        saved_df = pd.read_pickle(FILE_PATH)
        st.metric("ì €ì¥ëœ ìƒí™©ë³´ê³  ê±´ìˆ˜", f"{len(saved_df)} ê±´")
        
        st.markdown("**ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5ê±´):**")
        st.dataframe(saved_df.head(), use_container_width=True)
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
else:
    st.info("ì—…ë¡œë“œëœ ìƒí™©ë³´ê³  ì—‘ì…€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ì—‘ì…€ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")