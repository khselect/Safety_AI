[철도안전 AI 시스템 기술 명세서]

온프레미스 보안 아키텍처 (On-Premise Security Architecture)

✅ 기능 정의
외부 클라우드(OpenAI, Google 등)로 민감한 철도 데이터를 전송하지 않고, 로컬 환경(Localhost) 내에서 모든 AI 연산과 데이터 저장을 수행하는 보안 중심 아키텍처입니다.

⚙️ 기술적 메커니즘
1. Local LLM 구동: Ollama를 활용하여 Llama 3.1 등 고성능 오픈 소스 모델을 사내 서버/PC 메모리에 직접 로드하여 추론합니다.
2. Local Vector Store: ChromaDB를 로컬 파일 시스템(persist_directory)에 구축하여, 벡터 데이터가 외부 API를 타지 않고 내부 디스크에서만 입출력되도록 설계했습니다.
3. 네트워크 격리: 인터넷이 차단된 폐쇄망 환경에서도 requirements.txt 사전 설치 및 모델 파일 오프라인 로드를 통해 구동 가능합니다.

🎯 기대 효과

• 데이터 보안 절대 보장: 국가 주요 시설인 철도의 도면, 내부 규정, 사고 내역이 외부로 유출될 원천적인 가능성을 차단합니다.
• 운영 비용 절감: API 호출 당 비용이 발생하는 클라우드 방식과 달리, 초기 구축 후 추가적인 토큰 비용이 발생하지 않습니다. (전기세 제외 0원)

[소스트리]
<img width="624" height="474" alt="image" src="https://github.com/user-attachments/assets/49006ad0-f403-4baf-9847-99668669245f" />

[관리자 페이지]
<img width="1090" height="852" alt="image" src="https://github.com/user-attachments/assets/6cf9b1ba-2bda-46d2-9c5f-89248ffed224" />

[사용자 페이지]
<img width="1083" height="831" alt="image" src="https://github.com/user-attachments/assets/94d531a1-2754-438c-a187-60724041ea1f" />
<img width="1068" height="818" alt="image" src="https://github.com/user-attachments/assets/b99280fc-8050-4ebc-92ad-dbf4bca50a50" />
<img width="1039" height="789" alt="image" src="https://github.com/user-attachments/assets/444d0c92-35b4-44e7-b6cd-838e51bd01d0" />
<img width="1040" height="815" alt="image" src="https://github.com/user-attachments/assets/9a1a37c9-af5b-4239-afcf-1477c213493b" />
<img width="1012" height="798" alt="image" src="https://github.com/user-attachments/assets/cef4c507-2b4a-4e4f-9c1f-c26609439ab1" />

1. 스마트 소스 필터링 (Context-Aware Source Filtering)

✅ 기능 정의
사용자의 자연어 질의를 분석하여, 질문이 특정 규정 문서(예: 운전취급규정, 시설관리요령 등)를 지칭하는지 자동으로 파악하고, 검색 범위를 해당 문서로 한정하는 지능형 전처리 기술입니다.

⚙️ 기술적 메커니즘
1. 메타데이터 인덱싱: 업로드된 모든 규정 파일명에서 의미 있는 키워드(예: '보수', '운전', '비상')를 추출하여 인덱싱합니다.
2. 질의 매칭 알고리즘: 사용자가 "비상대응 절차 알려줘"라고 질문하면, 질문 내 '비상'이라는 키워드를 감지합니다.
3. 동적 필터링(Dynamic Filtering): Vector DB 검색 시 filter={"source": "비상대응매뉴얼.pdf"} 조건을 동적으로 주입하여, 관련 없는 문서(예: 인사규정, 복무규정 등)를 검색 후보에서 원천 배제합니다.
   
🎯 기대 효과
• 정확도(Accuracy) 향상: 동음이의어(다른 규정집에 있는 같은 용어)로 인한 혼동을 방지합니다.
• 검색 속도 최적화: 전체 DB를 스캔하는 대신 특정 문서 내에서만 검색하므로 응답 속도가 빨라집니다.

2. MMR 검색 (Maximal Marginal Relevance Search)
   
✅ 기능 정의
단순히 질문과 단어 유사도가 높은 문서를 찾는 것을 넘어, **문서 간의 다양성(Diversity)**을 고려하여 중복되지 않고 풍부한 정보를 제공하는 고도화된 검색 알고리즘입니다.

⚙️ 기술적 메커니즘
1. 벡터 유사도 계산: 사용자의 질문 벡터와 가장 가까운 후보군(예: 20개)을 1차로 선정합니다.
2. 다양성 재정렬 (Re-ranking):
• 이미 선택된 문서와 내용이 너무 유사한(중복된) 문서는 페널티를 부여합니다.
• Lambda 파라미터(0.7)를 적용하여 **관련성 70%, 다양성 30%**의 비율로 최적의 균형을 맞춥니다.
• 예) '작업 중지' 검색 시, 제1조 1항만 5개 나오는 것을 방지하고, 제1조(정의), 제5조(절차), 별표(기준) 등을 골고루 추출합니다.

🎯 기대 효과
• 정보 편향 방지: 특정 조항만 반복해서 보여주는 문제를 해결합니다.
• 문맥 파악력 증대: 관련된 여러 조항을 입체적으로 제시하여 AI가 더 논리적인 답변을 생성하도록 돕습니다.

3. 표/수치 정밀 보존 로직 (Table & Numerical Integrity Preservation)

✅ 기능 정의
철도 규정의 특성상 중요한 등급표, 수치, 제원 등이 LLM의 생성 과정에서 왜곡(Hallucination)되거나 텍스트로 뭉개지는 것을 방지하는 프롬프트 엔지니어링(Prompt Engineering) 기술입니다.

⚙️ 기술적 메커니즘
1. System Instruction 강화: LLM에게 "상상하지 말 것", "수치는 원문 그대로 인용할 것"을 시스템 레벨에서 강제합니다.
2. 마크다운(Markdown) 강제: 복잡한 규정의 표(Table) 데이터를 마크다운 포맷(| 구분 | 기준 | ... |)으로 출력하도록 지시하여, 가독성과 구조를 유지합니다.
3. 출처 명시 프로토콜: 답변의 모든 문장에 대해 "제OO조에 따르면"과 같이 근거를 명시하도록 유도하여 신뢰성을 확보합니다.
   
🎯 기대 효과
• 데이터 무결성(Integrity) 확보: 안전과 직결된 전압, 거리, 시간 등의 수치 오류를 차단합니다.
• 가독성 증대: 복잡한 규정표를 채팅창에서 깔끔하게 볼 수 있어 사용자 경험(UX)이 개선됩니다.

4. UI 역순 배치 및 대화 그룹화 (Chronological UI Layout)
   
✅ 기능 정의
사용자가 가장 최근의 문답을 즉시 확인할 수 있도록 대화 내역을 **최신순(Reverse Chronological)**으로 배치하고, 연속된 대화를 그룹화하여 시각적 피로도를 낮추는 사용자 경험(UX) 최적화 로직입니다.

⚙️ 기술적 메커니즘
1. 세션 상태 관리(Session State Management): Streamlit의 휘발성 데이터를 보존하기 위해 대화 객체를 구조화(role, content, timestamp)하여 메모리에 저장합니다.
2. 역순 렌더링(Reverse Rendering): reversed() 함수를 적용하여 새로운 질문과 답변이 화면 최상단(또는 컨테이너 상단)에 위치하게 합니다. 이는 긴 스크롤 없이 바로 결과를 확인하기 위함입니다.
3. 메시지 그룹화: 사용자가 연속으로 질문하거나 AI가 연속으로 답할 때, 이를 하나의 '블록'으로 묶어 채팅방처럼 자연스럽게 표현합니다.
   
🎯 기대 효과
• 업무 효율성 증대: 사용자가 스크롤을 내릴 필요 없이 가장 중요한 '최신 결과'를 즉시 확인할 수 있습니다.
• 직관적인 인터페이스: 카카오톡 등 익숙한 메신저 UI 패턴을 차용하여 별도의 교육 없이 사용 가능합니다.
 
5. 이기종 문서 통합 파싱 엔진 (Multi-Format Document Parsing Engine)
   
✅ 기능 정의
PDF, DOCX뿐만 아니라, 국내 공공기관 및 철도 현장에서 표준으로 사용하는 HWP(한글) 포맷과 비정형 텍스트를 시스템이 이해할 수 있는 표준 마크다운(Markdown) 포맷으로 변환·통합하는 전처리 모듈입니다.

⚙️ 기술적 메커니즘
1. HWP 딥 다이빙(Deep Diving): olefile 라이브러리를 사용하여 HWP 파일의 바이너리 구조(OLE Storage)에 직접 접근, 텍스트 스트림(PrvText)을 추출하고 유니코드(UTF-16LE)로 디코딩하여 텍스트 손실을 최소화합니다.
2. 마크다운 정규화: 추출된 텍스트에서 불필요한 공백, 특수문자를 제거하고 규정의 계층 구조(장, 절, 조)를 식별하여 # (Header) 태그를 자동으로 부여합니다.
3. 범용 호환성: pymupdf(PDF), mammoth(DOCX) 등 파일 포맷별 최적화된 파서를 라우팅하여 단일 진입점(load_file)으로 통합 관리합니다.
   
🎯 기대 효과
• 데이터 사각지대 해소: 기존 AI 시스템에서 처리가 어려웠던 HWP 기반의 '구형 매뉴얼'이나 '현장 작업 지침서'를 자산화할 수 있습니다.
• 학습 데이터 표준화: 모든 문서를 통일된 포맷으로 변환하여 AI 학습 효율을 극대화합니다.

6. 하이브리드 청킹 전략 (Hybrid Chunking Strategy)
   
✅ 기능 정의
방대한 규정집을 AI가 읽기 좋은 크기로 자를 때, 단순히 글자 수로 자르는 것이 아니라 문맥(조항, 별표 등) 단위로 의미를 보존하며 분할하는 지능형 분할 기술입니다.

⚙️ 기술적 메커니즘
1. 1차 의미론적 분할 (Semantic Splitting): 정규표현식(Regex)을 통해 제OO조, [별표], [별지] 등의 패턴을 감지하여, 하나의 조항이 중간에 잘리지 않도록 1차 그룹핑을 수행합니다. (MarkdownHeaderTextSplitter 활용)
2. 2차 토큰 제한 분할: 1차 분할된 덩어리가 LLM의 입력 한계(Context Window)를 초과할 경우에만 RecursiveCharacterTextSplitter를 적용하여 문맥 유실을 최소화하며 미세 조정합니다.
   
🎯 기대 효과
• 검색 정확도 향상: "제5조에 따르면..."과 같은 답변 생성 시, 제5조의 내용이 여러 청크로 쪼개져 누락되는 현상을 방지합니다.
• 문맥 보존: AI가 앞뒤 문맥(조항의 전제 조건 등)을 완벽하게 이해하고 답변할 수 있습니다.

7. 맥락 기반 위험성 평가 (Context-Aware Risk Assessment / Decision AI)
   
✅ 기능 정의
단순한 정보 검색(Tab 1)을 넘어, 과거 사고 데이터(정형 데이터)와 관련 규정(비정형 데이터)을 결합하여 사고의 위험 등급을 판정하고 구체적인 조치사항을 제안하는 의사결정 지원 모듈입니다.

⚙️ 기술적 메커니즘
1. 크로스 데이터 퓨전 (Cross-Data Fusion):
• Input A: 사용자가 선택한 과거 사고 사례 (일시, 장소, 원인, 피해 규모 등 정형 데이터)
• Input B: 해당 사고와 관련된 안전 규정 및 매뉴얼 (RAG로 실시간 검색된 비정형 데이터)
2. LLM 추론 (Reasoning): AI에게 **"안전 관리자 페르소나"**를 부여하고, A와 B를 종합하여 다음 3가지를 도출하도록 프롬프트 엔지니어링 수행:
• 위험도 등급: High / Medium / Low
• 판단 근거: 규정 제OO조 위반 여부 및 유사 사례 분석
• 즉각 조치사항: 현장에서 바로 실행해야 할 행동 수칙
3. 구조화된 출력: LLM의 줄글 출력을 JSON 형태로 파싱하여 UI에 시각화(등급에 따른 색상 코딩 등)합니다.
   
🎯 기대 효과
• 의사결정 표준화: 관리자의 경험에 의존하던 위험성 평가를 규정 데이터 기반으로 객관화합니다.
• 선제적 예방: 단순 사고 기록 조회를 넘어, "이런 상황에서는 어떤 조치를 취해야 하는가?"에 대한 Action Item을 즉시 제공합니다.



